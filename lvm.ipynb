{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LVM - large vector model\n",
    "\n",
    "## Abstract\n",
    "Large vector model (LVM) is a generalization of the LLM class of models used in language generation.\n",
    "LVMs are transformer-based models designed for prediction of temporal sequences with tokens.\n",
    "However, unlike LLMs that generate human-language tokens, LVM can generate a sequence of R^n vectors\n",
    "or a mixture of aligned tokens-vectors.\n",
    "LVM model promise to help in prediction of time series data in a variety of significant applications.\n",
    "In particular, it provides an important generalization of binary classification problems involving extended history.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief Introduction\n",
    "The transformer architecture in deep neural networks has been applied widely beyond its original use for prediction human language sequence. In particular, a sequence of studies applied transformers for forecasting of time-series data (either regular or irregular). A pure vector sequence might represent daily stock prices for 500 stocks over 300 days and the goal is to predict the prices in the following T days.\n",
    "\n",
    "This project introduces a novel generalization of transformers for time-series tabular data with aligned word sequences. In the original transformer the input consists of a sequence of length w from a dictionary D of tokens $(x_1...x_w) \\in D^w$. Here we suppose that at each of the w points we receive a vector and a token, and thus: $(x_1...x_w) \\in (\\cal{R} +D)^w$.  By contrast, the classic time-series prediction has $D=\\emptyset$.\n",
    "\n",
    "\n",
    "There are numerous interesting applications of this\n",
    "- human-produced words but where at each token we *also* measure characteristics such as volume, emotion (suitably-quantified) etc\n",
    "- prediction of next-label from sequence data: say, a physical system (a plant, a natural phenomenon) where at each day its state is described by state vector + categorical label (binary or N-ary) and the goal is to predict the label in t+1\n",
    "- general situations normally solved by hidden-markov models where tokens are produced by states in $R^n$\n",
    "\n",
    "Novelty: \n",
    "- whereas there are several successful implementations of time-series transformer models, here we examine generation to include a token label  \n",
    "- most importantly, when the token is a binary label, the approach promises to improve the solution of binary classification problem where the input vectors have extended history that varies from sample to sample.  \n",
    "\n",
    "## Related studies\n",
    "- Liu et al. 2023 - [iTransformer: Inverted Transformers Are Effective for Time Series Forecasting](https://arxiv.org/abs/2310.06625)\n",
    "- Ma et al. 2023 - [BTAD: A binary transformer deep neural network model for anomaly detection in multivariate time series data](https://www.sciencedirect.com/science/article/abs/pii/S1474034623000770?via%3Dihub)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "In a previous unpublished study, the authors have developed a simple generalization of transformers to binary state prediction in healthcare management, i.e. the goal was to predict the label in $t+1$ where $D=\\{0,1\\}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code\n",
    "Code sources\n",
    "- [Aladdin Persson's transformer from scratch](https://github.com/aladdinpersson/Machine-Learning-Collection/blob/558557c7989f0b10fee6e8d8f953d7269ae43d4f/ML/Pytorch/more_advanced/transformer_from_scratch/transformer_from_scratch.py).  Under the [MIT License](https://github.com/aladdinpersson/Machine-Learning-Collection/blob/558557c7989f0b10fee6e8d8f953d7269ae43d4f/LICENSE.txt)\n",
    "- standard dataloader examples \n",
    "- Github Copilot\n",
    "\n",
    "WIP\n",
    "- data loader, e.g. using torch lightning\n",
    "- training code, pretraining + use torch lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requirements\n",
    "# torch - select your hardware, CUDA version and OS \n",
    "\n",
    "#pip install positional-encodings==6.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pdb\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import sys, math, copy\n",
    "import time\n",
    "import warnings\n",
    "from typing import Tuple\n",
    "from tempfile import TemporaryFile\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import dataset, DataLoader\n",
    "\n",
    "\n",
    "from positional_encodings.torch_encodings import PositionalEncoding1D, Summer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model architecture\n",
    "\n",
    "This is a decoder-only transformer model\n",
    "1. input sentence containing vectors + words\n",
    "2. Positional Encoder\n",
    "3. Masked multi-headed attention "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Emebdding size needs to be divisible by heads\"\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "        values = self.values(values)  #(N, value_len, embed_size)\n",
    "        keys = self.keys(keys) #(N, key_len, embed_size)\n",
    "        queries = self.queries(query) #(N, query_len, embed_size)\n",
    "\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = queries.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        #einstein summation for tensor multiplication\n",
    "        energy = torch.einsum(\"nqhd, nkhd->nhqk\", [queries, keys])\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1/2)), dim=3)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nhld->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads*self.head_dim\n",
    "        )\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "\n",
    "        return out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion*embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "\n",
    "        x= self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "                self,\n",
    "                word_embed_size,\n",
    "                vector_embed_size,\n",
    "                num_layers,\n",
    "                heads,\n",
    "                trg_vocab_size,\n",
    "                forward_expansion,\n",
    "                dropout,\n",
    "                device,\n",
    "                max_length):\n",
    "            super(Decoder, self).__init__()\n",
    "            self.device = device\n",
    "            self.embed_size = vector_embed_size + word_embed_size\n",
    "            \n",
    "            #TODO: implement more intelligent embedding - currently it just randomizes. e.g. PositionalEncoding1d\n",
    "            self.word_embedding = nn.Embedding(trg_vocab_size, word_embed_size)\n",
    "            self.position_embedding = nn.Embedding(max_length, self.embed_size)\n",
    "\n",
    "            self.layers = nn.ModuleList(\n",
    "                [\n",
    "                    TransformerBlock(embed_size=self.embed_size, heads=heads,\n",
    "                                    dropout=dropout, forward_expansion=forward_expansion)\n",
    "                                    for _ in range(num_layers)\n",
    "                ]\n",
    "            )\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            self.fc_out = nn.Linear(self.embed_size, 1)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def embed(self, vec):\n",
    "         #TODO: implement something like\n",
    "         #return self.dropout((self.word_embedding(vec[0]) + self.position_embedding(vec[1:]))\n",
    "         pass\n",
    "    \n",
    "    def forward(self, x_input, trg_mask):\n",
    "        out = self.dropout(self.embed(x_input))\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out, trg_mask) \n",
    "\n",
    "        out = self.fc_out(out) #sigmoid - binary classification. in general need to train by self-supervise\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "the main class for the LVM\n",
    "\n",
    "it assumes that the x[0] is the word, and the rest is real vectorized data\n",
    "\n",
    "parameters\n",
    "    word_embed_size \n",
    "    vector_embed_size\n",
    "    num_layers\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "class LVM(nn.Module):\n",
    "    def __init__(self, word_embed_size=512, vector_embed_size=100, num_layers=6, forward_expansion=4, heads=8, dropout=0, device='cpu', trg_vocab_size=1000, max_length=100):\n",
    "        super(LVM, self).__init__()\n",
    "\n",
    "        self.decoder = Decoder(word_embed_size=word_embed_size, vector_embed_size=vector_embed_size,\n",
    "                               num_layers=num_layers, heads=heads, forward_expansion=forward_expansion,\n",
    "                                dropout=dropout, device=device, trg_vocab_size=trg_vocab_size, max_length=max_length)\n",
    "        self.device = device\n",
    "        #self._init_weights()\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        N, trg_len, _ = trg.shape\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "            N, 1, trg_len, trg_len\n",
    "        )\n",
    "        return trg_mask.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        trg_mask = self.make_trg_mask(x)\n",
    "        out = self.decoder(x, trg_mask)\n",
    "\n",
    "        return torch.sigmoid(out)  #TODO: this is a binary classification. typically pre-train for self-supervision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.BCELoss(,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
