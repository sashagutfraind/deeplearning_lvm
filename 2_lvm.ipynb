{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LVM - language-vector model\n",
    "\n",
    "## Abstract\n",
    "LVMs are transformer-based models designed for prediction of temporal sequences with tokens.\n",
    "Unlike LLMs that generate only tokens, LVM can generate tokens aligned with a sequence of R^n vectors\n",
    "or a mixture of aligned tokens-vectors.\n",
    "LVM model is useful for prediction of time series data in a variety of significant applications.\n",
    "For example, it can be applied for binary classification problems involving extended history.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief Introduction\n",
    "The transformer architecture in deep neural networks has been applied widely beyond its original use for prediction human language sequence. In particular, a sequence of studies applied transformers for forecasting of time-series data (either regular or irregular). A pure vector sequence might represent daily stock prices for 500 stocks over 300 days and the goal is to predict the prices in the following T days.\n",
    "\n",
    "This project introduces a novel generalization of transformers for time-series tabular data with aligned word sequences. In the original transformer the input consists of a sequence of length w from a dictionary D of tokens $(x_1...x_w) \\in D^w$. Here we suppose that at each of the w points we receive a vector and a token, and thus: $(x_1...x_w) \\in (\\cal{R} +D)^w$.  By contrast, the classic time-series prediction has $D=\\emptyset$.\n",
    "\n",
    "\n",
    "There are numerous interesting applications of this\n",
    "- human-produced words but where at each token we *also* measure characteristics such as volume, emotion (suitably-quantified) etc\n",
    "- prediction of next-label from sequence data: say, a physical system (a plant, a natural phenomenon) where at each day its state is described by state vector + categorical label (binary or N-ary) and the goal is to predict the label in t+1\n",
    "- general situations normally solved by hidden-markov models where tokens are produced by states in $R^n$\n",
    "\n",
    "Novelty: \n",
    "- whereas there are several successful implementations of time-series transformer models, here we examine generation to include a token label  \n",
    "- most importantly, when the token is a binary label, the approach promises to improve the solution of binary classification problem where the input vectors have extended history that varies from sample to sample.  \n",
    "\n",
    "\n",
    "# Extensions\n",
    "\n",
    "**Irregular Time** - The vanila architecture does not specify the time separation between the vectors.  This is sufficient if the original data is regular or naturally clusters into weeks or months. It is easy to generalize to data that has time separation by adding a time variable as an added vector dimension (see Luo, Ye et al. 2020). \n",
    "\n",
    "## Related studies\n",
    "- Liu et al. 2023 - [iTransformer: Inverted Transformers Are Effective for Time Series Forecasting](https://arxiv.org/abs/2310.06625)\n",
    "- Ma et al. 2023 - [BTAD: A binary transformer deep neural network model for anomaly detection in multivariate time series data](https://www.sciencedirect.com/science/article/abs/pii/S1474034623000770?via%3Dihub)\n",
    "- Kotelnikov, Baranchuk et al.[TabDDPM] in ICML 2023\n",
    "- Luo, Ye et al. Hitanet: hierarchical time-aware attention networks for risk prediction SIGKDD 2020\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "In a previous unpublished study, the authors have developed a simple generalization of transformers to binary state prediction in healthcare management, i.e. the goal was to predict the label in $t+1$ where $D=\\{0,1\\}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code\n",
    "Code sources\n",
    "- [Aladdin Persson's transformer from scratch](https://github.com/aladdinpersson/Machine-Learning-Collection/blob/558557c7989f0b10fee6e8d8f953d7269ae43d4f/ML/Pytorch/more_advanced/transformer_from_scratch/transformer_from_scratch.py).  Under the [MIT License](https://github.com/aladdinpersson/Machine-Learning-Collection/blob/558557c7989f0b10fee6e8d8f953d7269ae43d4f/LICENSE.txt)\n",
    "- standard dataloader, lightning examples \n",
    "- Github Copilot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pdb\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import sys, math, copy\n",
    "import time\n",
    "import warnings\n",
    "from typing import Tuple\n",
    "\n",
    "#debugging\n",
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import dataset, DataLoader\n",
    "\n",
    "import lightning as L\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretraining_data = r'data\\synthetic1seed=0_n=1000.pkl' #for debugging only\n",
    "pretraining_data = r'data\\synthetic1seed=0_n=100000.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model architecture\n",
    "\n",
    "This is a decoder-only transformer model\n",
    "1. input sentence containing vectors + words\n",
    "2. Positional Encoder\n",
    "3. Masked multi-headed attention "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Emebdding size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(embed_size, embed_size)\n",
    "        self.keys = nn.Linear(embed_size, embed_size)\n",
    "        self.queries = nn.Linear(embed_size, embed_size)\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "        values = self.values(values)  #(N, value_len, embed_size)\n",
    "        keys = self.keys(keys) #(N, key_len, embed_size)\n",
    "        queries = self.queries(query) #(N, query_len, embed_size)\n",
    "\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = queries.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        #einstein summation for tensor multiplication\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1/2)), dim=3)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads*self.head_dim\n",
    "        )\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "\n",
    "        return out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion*embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "\n",
    "        x= self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code from https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(1)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder_Embedder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder with an embedded embedder for the tokens\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "                self,\n",
    "                word_embed_size,\n",
    "                vector_embed_size,\n",
    "                num_layers,\n",
    "                heads,\n",
    "                vocab_size,\n",
    "                forward_expansion,\n",
    "                dropout,\n",
    "                device,\n",
    "                max_length):\n",
    "            super(Decoder_Embedder, self).__init__()\n",
    "            self.device = device\n",
    "            self.embed_size = vector_embed_size + word_embed_size\n",
    "            self.vector_embed_size = vector_embed_size\n",
    "\n",
    "            #it's recommended to use more intelligent embedding - currently it just randomizes. e.g. PositionalEncoding1d\n",
    "            self.word_embedding = nn.Embedding(vocab_size, word_embed_size)   \n",
    "\n",
    "            self.pos_encoder    = PositionalEncoding(d_model=self.embed_size, dropout=dropout, max_len=max_length)\n",
    "\n",
    "            self.layers = nn.ModuleList(\n",
    "                [\n",
    "                    TransformerBlock(embed_size=self.embed_size, heads=heads,\n",
    "                                    dropout=dropout, forward_expansion=forward_expansion)\n",
    "                                    for _ in range(num_layers)\n",
    "                ]\n",
    "            )\n",
    "            self.retokenizer = nn.Linear(self.word_embed_size, 1)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        #debug x[-1]: something like this:\n",
    "        embedded_token = self.word_embedding(x[:,:,-1].long()).unsqueeze(1) \n",
    "        embedded_all   = torch.cat((x[:,:,:-1], embedded_token), dim=1) \n",
    "        out = self.pos_encoder(embedded_all)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out, mask) \n",
    "\n",
    "        out2 = torch.cat(out[:,-1,:self.vector_embed_size], \n",
    "                         self.retokenizer(out[:,-1,self.vector_embed_size:]))\n",
    "        \n",
    "        return out2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder_Simple(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder without the embedding of token\n",
    "    \"\"\" \n",
    "    def __init__(\n",
    "                self,\n",
    "                word_embed_size,\n",
    "                vector_embed_size,\n",
    "                num_layers,\n",
    "                heads,\n",
    "                vocab_size,\n",
    "                forward_expansion,\n",
    "                dropout,\n",
    "                device,\n",
    "                max_length):\n",
    "            super(Decoder_Simple, self).__init__()\n",
    "            self.device = device\n",
    "            self.embed_size = vector_embed_size + word_embed_size\n",
    "            \n",
    "            self.pos_encoder    = PositionalEncoding(d_model=self.embed_size, dropout=dropout, max_len=max_length)\n",
    "\n",
    "            self.layers = nn.ModuleList(\n",
    "                [\n",
    "                    TransformerBlock(embed_size=self.embed_size, heads=heads,\n",
    "                                    dropout=dropout, forward_expansion=forward_expansion)\n",
    "                                    for _ in range(num_layers)\n",
    "                ]\n",
    "            )\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            #for binary classification \n",
    "            # self.fc_out = nn.Linear(self.embed_size, 1)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        out = self.pos_encoder(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out, mask) \n",
    "\n",
    "        #for binary classification\n",
    "        #out = self.fc_out(out) \n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "the main class for the LVM\n",
    "\n",
    "it assumes that the x[:,-1] is the word, and the rest is real vectorized data\n",
    "\n",
    "core parameters\n",
    "    word_embed_size \n",
    "    vector_embed_size\n",
    "    num_layers\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "class LVM(nn.Module):\n",
    "    def __init__(self, decoder='simple', word_embed_size=512, vector_embed_size=10, num_layers=6, forward_expansion=4, heads=8, dropout=0, device='cpu', vocab_size=1000, max_length=100):\n",
    "        super(LVM, self).__init__()\n",
    "\n",
    "        if decoder == 'simple':\n",
    "            decoder = Decoder_Simple\n",
    "        else:\n",
    "            decoder = Decoder_Embedder\n",
    "        self.decoder = decoder(word_embed_size=word_embed_size, vector_embed_size=vector_embed_size,\n",
    "                            num_layers=num_layers, heads=heads, forward_expansion=forward_expansion,\n",
    "                                dropout=dropout, device=device, vocab_size=vocab_size, max_length=max_length)\n",
    "        self.device = device\n",
    "        #self._init_weights()\n",
    "\n",
    "    def make_mask(self, N, x_len):\n",
    "        \"\"\"\n",
    "        mask the triangular mask with 1 to mask the next words in the sequence\n",
    "        we can't cache it because in general the N can very from input to input\n",
    "        \"\"\"\n",
    "        mask = torch.tril(torch.ones((x_len, x_len))).expand(\n",
    "            N, 1, x_len, x_len\n",
    "        )\n",
    "        return mask.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = self.make_mask(N=x.shape[0], x_len=x.shape[1]) \n",
    "        out = self.decoder(x, mask)\n",
    "\n",
    "        return out\n",
    "        #return torch.sigmoid(out)  #this is a binary classification. typically pre-train for self-supervision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#caution: the model should be pre-trained to just before loss falls to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_synth_fpath = pretraining_data\n",
    "dataset_synth_vocab_size = 11 #10 regular and +1 is for padding value\n",
    "dataset_synth_index_token_val = 22\n",
    "vector_embed_size = dataset_synth_index_token_val-1 #all but the last column\n",
    "\n",
    "heads = 8\n",
    "vector_embed_size = dataset_synth_index_token_val-1\n",
    "word_embed_size1 = min(dataset_synth_vocab_size, 200)\n",
    "word_embed_size2 = word_embed_size1 + (heads - ((vector_embed_size + word_embed_size1) % heads)) % 8\n",
    "\n",
    "# load the data with pickle:\n",
    "with open(dataset_synth_fpath, 'rb') as f:\n",
    "    dataset_synth = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LVM design options\n",
    "1. Send the mixed data through the transformer, including the categorical token + real-valued vectors\n",
    "    a. part of the output needs to go through embedding at the input and output layers\n",
    "    b. the loss function mixes types e.g. MSE and CrossEntropy\n",
    "2. Embed the tokens before sending the data as real-valued vecotrs  \n",
    "    a. Dataset needs pre-processing before the transformer\n",
    "    b. Fine-tuning step will involve additional adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## write a torch.utils.data.TensorDataset class to wrap the tensor.  dim 0 is the sample ID\n",
    "class TensorDataset(torch.utils.data.TensorDataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index, :, :]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorDatasetEmbedder(torch.utils.data.TensorDataset):\n",
    "    def __init__(self, data, embedder=None, word_embed_size=None, vocab_size=None):\n",
    "        self.data = data\n",
    "        self.device=data.device\n",
    "        if embedder is not None:\n",
    "            self.embedder = embedder\n",
    "        else:\n",
    "            self.embedder = nn.Embedding(vocab_size, word_embed_size, device=self.device)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index, :, :]\n",
    "        embedded_token = self.embedder(x[:,-1].long())\n",
    "        embedded_all   = torch.cat((x[:,:-1], embedded_token), dim=1) \n",
    "        return embedded_all\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_synth2 = TensorDatasetEmbedder(dataset_synth, \n",
    "                                       word_embed_size=word_embed_size2, \n",
    "                                       vocab_size=dataset_synth_vocab_size)\n",
    "dataset_synth2_train_loader = DataLoader(dataset_synth2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitAutoEncoder(L.LightningModule):\n",
    "    def __init__(self, lvm):\n",
    "        super().__init__()\n",
    "        self.lvm = lvm\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        x = batch\n",
    "        x2 = x.view(x.size(0), -1)\n",
    "        x_hat = self.lvm(x)\n",
    "        x_hat2 = x_hat.view(x.size(0), -1)\n",
    "        loss = F.mse_loss(x_hat2, x2)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvm1 = LVM(decoder='simple',\n",
    "          vocab_size=dataset_synth_vocab_size, \n",
    "            vector_embed_size=vector_embed_size, \n",
    "            word_embed_size=word_embed_size2,\n",
    "            heads=heads,\n",
    "            max_length=dataset_synth.shape[1],\n",
    "            device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(embedded_all.shape)\n",
    "\n",
    "#lvm1(dataset_synth2[0].to(lvm1.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type | Params\n",
      "------------------------------\n",
      "0 | lvm  | LVM  | 76.2 K\n",
      "------------------------------\n",
      "76.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "76.2 K    Total params\n",
      "0.305     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/1000 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:  31%|███▏      | 313/1000 [00:07<00:17, 40.17it/s, v_num=4] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\agutf\\miniforge3\\envs\\lvm\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "autoencoder = LitAutoEncoder(lvm=lvm1)\n",
    "\n",
    "# train model\n",
    "trainer = L.Trainer()\n",
    "trainer.fit(model=autoencoder, train_dataloaders=dataset_synth2_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
